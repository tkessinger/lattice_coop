\documentclass[14pt, a4paper, justified]{article}

\usepackage{amsfonts,latexsym,amsthm,amssymb,amsmath,amscd,euscript}
%\usepackage{framed}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{mathtools}
%\usepackage{charter}
%\usepackage{libertine} \usepackage[libertine]{newtxmath}
%\usepackage{newpxtext, newpxmath}
%\usepackage{libertine}
%\usepackage{fourier} \usepackage[euler-digits]{eulervm}
%\usepackage{fourier} \usepackage{newpxmath}
%\usepackage{kpfonts, mathpazo}
\usepackage[utopia]{mathdesign}
%\usepackage{utopia} \usepackage{newpxmath}
%\usepackage{kpfonts}
%\usepackage{newpxtext} \usepackage[euler-digits]{eulervm}
\usepackage{natbib}

\usepackage[dvipsnames]{xcolor}
\usepackage[usenames,dvipsnames]{pstricks}

\usepackage{hyperref}
\hypersetup{
    pdffitwindow=false,            % window fit to page
    pdfstartview={Fit},            % fits width of page to window
    pdftitle={Spatial replicator equation notes},     % document title
    pdfauthor={Taylor Kessinger},         % author name
    pdfsubject={},                 % document topic(s)
    pdfnewwindow=true,             % links in new window
    colorlinks=true,               % coloured links, not boxed
    linkcolor=OrangeRed,      % colour of internal links
    citecolor=ForestGreen,       % colour of links to bibliography
    filecolor=Orchid,            % colour of file links
    urlcolor=Cerulean           % colour of external links
}

\begin{document}

\section*{Replicator equation notes}

We'll proceed to try and derive the replicator equation from \citet{ohtsuki.nowak_2006}, which is
\begin{equation}
    \dot{x}_i = x_i\left[ \sum_{j=1}^n x_j(a_{ij} + b_{ij}) - \phi \right].
\end{equation}
To get there, we need to first consider how the conditional frequencies of each type $q_{i|j} = x_{ij}/x_j$ behave: we'll assume those equilibrate quickly in response to changes in the actual frequencies $x_i$.
We'll use the pair approximation, which is simply to say that $q_{i|jl} = q_{i|j}$, i.e., the behavior of a neighbor two steps away playing strategy $l$ does not affect the $ij$ frequencies.

The first equation we need to derive is this monster from \citet{ohtsuki.nowak_2006}:
\begin{equation}
    \dot{q}_{i|j} = \frac{\dot{x}_{ij}}{x_j} = \frac{2}{k}\left[ \overbrace{\delta_{ij}}^{\textrm{$i=j$ correction}} + \underbrace{(k-1)\left( \sum_l q_{i|l}q_{l|j} \right)}_{\textrm{pairs created}} - \overbrace{kq_{i|j}}^{\textrm{pairs destroyed}} \right] + O(w),
    \label{eq:bd_update}
\end{equation}
where $O(w)$ is because we're assuming weak selection (hence why $x_j$ is treated as constant) and $k$ is the degree of the graph.
Let's see if we can figure out where this equation comes from.

\subsection{Two-strategy case}

First, let's consider the case where there are only two strategies: $i$ and $j$.
With probability $x_j$, a $j$ individual is chosen to reproduce.
It replaces an $i$ neighbor with probability $q_{i|j}$.
Said $i$ neighbor will have $(k-1)q_{i|i}$ $i$ neighbors, so an average of
$(k-1)q_{i|j}q_{i|i}$ (the $i$ term in the ``pairs created'' sum) $ij$ pairs are created as a result.
However, that $i$ neighbor will itself have been part of $1 + (k-1)q_{j|i}$ $ij$ pairs, so an average of $q_{i|j}[ (k-1)q_{j|i} + 1 ]$ $ij$ pairs are destroyed as a result.

It replaces a $j$ neighbor with probability $q_{j|j}$.
There had better be no change in $x_{ij}$ as a result of this.
That $j$ neighbor will have $(k-1)q_{i|j}$ $i$ neighbors, so an average of $q_{j|j}(k-1)q_{i|j}$ (the $j$ term in the ``pairs created'' sum) pairs are ``created''.
These are ``fictitious'' pairs that we will have to ``destroy'' later to avoid any net change in $x_{ij}$; that is, effectively $(k-1)q_{j|j}q_{i|j}$ pairs are ``destroyed'' in turn.
Combining this with the above expression for the number of pairs that are destroyed when $j$ replaces $i$ yields $q_{i|j} [ (k-1)q_{j|i} + 1 + (k-1)q_{j|j} ]$.

Now suppose an $i$ individual is chosen, with probability $x_i$.
It replaces a $j$ neighbor with probability $q_{j|i}$.
Using the same argument as above, we have that $(k-1)q_{j|i}q_{j|j}$ $ij$ pairs are created, and in turn $q_{j|i}[ (k-1)q_{i|j} + 1 ]$ are destroyed.

$i$ replaces an $i$ neighbor with probability $q_{i|i}$.
It has $(k-1)q_{j|i}$ $j$ neighbors, so $q_{i|i}(k-1)q_{j|i}$ pairs are ``created''.
As before, these are fictitious pairs that will have to be destroyed.
Combining this with the above destruction term yields $q_{j|i}[ (k-1)q_{i|j} + 1 + (k-1)q_{i|i} ]$.
Note that this is the expression for the case where an $i$ individual is picked, so technically it has an $x_i$ prefactor: we can rescale via $q_{j|i} = q_{i|j}x_j/x_i$.
The ``pair creation'' terms can thus be mashed together: we have
\begin{equation}
    x_j \sum_l q_{l|j} q_{i|l} + x_i \sum_l q_{l|i} q_{j|l} = 2 x_j \sum_l q_{l|j} q_{i|l},
    \label{eq:pair_creation}
\end{equation}
using $x_i q_{l|i} q_{j|l} = x_l q_{i|l} q_{j|l} = x_j q_{i|l} q_{l|j}$.

The total rate at which pairs are destroyed thus becomes
\begin{equation}
x_j q_{i|j} [ (k-1)q_{j|i} + 1 + (k-1)q_{j|j} ]
+ x_i q_{j|i} [ (k-1)q_{i|j} + 1 + (k-1)q_{i|i} ].
\end{equation}
We will want to get all of this in terms of $x_j$.
The second term becomes $x_j q_{i|j} [ (k-1)q_{i|j} + 1 + (k-1)q_{i|i} ]$.
So in total, we have
\begin{equation}
    \begin{split}
        & x_j q_{i|j} [ (k-1)q_{j|i} + 1 + (k-1)q_{j|j} ]
        + x_j q_{i|j}[ (k-1)q_{i|j} + 1 + (k-1)q_{i|i} ]
        \\
        & =  x_j q_{i|j} [ 2 + (k-1) (q_{j|i} + q_{i|i}) + (k-1) (q_{j|j} + q_{i|j} )]
        \\
        & =  2x_j kq_{i|j}.
    \end{split}
    \label{eq:pair_destruction_ij}
\end{equation}
We can put equations \ref{eq:pair_creation} and \ref{eq:pair_destruction_ij} together to obtain a simplified version of equation \ref{eq:bd_update}.
Keep in mind that we will divide out a factor of $2$, as implicitly we've been considering both $ij$ and $ji$ pairs ($x_{ij} = x_{ji}$, but we should avoid double counting).
There are $Nk/2$ pairs (with $N$ the total number of individuals in the network).
We thus have
\begin{equation}
    \begin{split}
        \frac{Nk}{2}\Delta x_{ij} & = \frac{1}{2} x_j \Big[2 (k-1) \sum_l \Big( q_{i|l}q_{l|j} \Big) - 2 kq_{i|j} \Big]
        \\
        \therefore \frac{\Delta x_{ij}}{x_j} & = \frac{2}{Nk} \Big[(k-1)\sum_l \Big( q_{i|l}q_{l|j} \Big) - kq_{i|j} \Big]
        \\
        \therefore \dot{q}_{i|j} = \frac{\dot{x}_{ij}}{x_j} & = \frac{2}{k} \Big[(k-1)\sum_l \Big(  q_{i|l}q_{l|j} \Big) - kq_{i|j} \Big],
    \end{split}
    \label{eq:bd_update_ij}
\end{equation}
whereupon we've sent the time increment to $0$ but assumed that each individual reproduces, on average, once during each increment (so the $N$ term disappears).

\subsection{Special case: $i = j$}

There is yet another special case to consider: what happens if $j = i$, i.e., we are concerned with $\dot{x}_{ii}$?
In that case, most of the above analysis holds.
With probability $x_i$, an $i$ individual is chosen to reproduce.
Call the other strategy $m$ to avoid notational confusion ($l$ will remain our index variable).
With probability $q_{l|i}$, an $l$ individual will be chosen for replacement, and $(k-1)\sum_l (q_{l|i}q_{i|l}) + q_{m|i}$ $ii$ pairs are created.
The extra $q_{m|i}$ term comes from the fact that, if $i$ populates an $m$ individual, that $i$ individual is now part of one extra $ii$ pair beyond what's already counted in the sum.
The $l = i$ term in that sum corresponds to the case where an $i$ individual is picked, which means $(k-1)q_{i|i}^2$ $i$ pairs are created.
However, again, those are ``fictitious'' pairs that we had better get rid of: nothing had better happen to $x_{ii}$ in that case.

We need to consider what happens when an $m$ individual is chosen to reproduce (with probability $x_m$).
No $ii$ pairs can be created.
However, $ii$ pairs can be destroyed: with probability $q_{i|m}$, an $i$ individual is chosen for replacement, who was in turn part of $(k-1)q_{i|i}$ $ii$ pairs.
The total change in $ii$ pairs is thus
\begin{equation}
    \begin{split}
        \frac{Nk}{2} \Delta x_{ii} & = x_i (k-1)\sum_l \Big( q_{l|i}q_{i|l} \Big) + x_i q_{m|i} - x_i(k-1)q_{i|i}^2 - x_m (k-1) q_{i|m}q_{i|i}
        \\
        & = x_i (k-1) \sum_l \Big( q_{l|i}q_{i|l} \Big) + x_i q_{m|i} - x_i (k-1) (q_{i|i}^2 + q_{m|i}q_{i|i})
        \\
        & = x_i (k-1) \sum_l \Big( q_{l|i}q_{i|l} \Big) + x_i q_{m|i} - x_i (k-1)q_{i|i}
        \\
        & = x_i (k-1) \sum_l \Big( q_{l|i}q_{i|l} \Big) + x_i (1 - q_{i|i}) - x_i (k-1)q_{i|i}
        \\
        & = x_i \Big[ 1 + (k-1) \sum_l \Big( q_{l|i}q_{i|l} \Big) - kq_{i|i} \Big],
    \end{split}
    \label{eq:bd_update_ii}
\end{equation}
using $1 - q_{m|i} = q_{i|i}$.
Note that there'll be no nagging factor of $2$ to divide out, as we haven't been double counting this time.
This can be made consistent with equation \ref{eq:bd_update_ij} by simply replacing $1$ with $\delta_{ij}$, so we recover equation \ref{eq:bd_update}.

\subsection{General case}

If we extend this to more than two strategies, an individual with arbitrary non-$i$, non-$j$ strategy (call it $m$ again) reproduces with probability $x_m$.
No $ij$ pairs can be created as a result, and equation \ref{eq:pair_creation} does not change (we simply allow $l$ to range over the $m$ strategies as well).
However, $ij$ pairs can be destroyed, so we may have to amend equation \ref{eq:pair_destruction_ij}.
$m$ can replace an $i$ individual, which happens with probability $q_{i|m}$.
That $i$ individual will have been part of $(k-1)q_{j|i}$ $ij$ pairs.
Likewise, $m$ can replace a $j$ individual with probability $q_{j|m}$, destroying $(k-1)q_{i|j}$ $ij$ pairs.

The total rate at which pairs are destroyed thus becomes
\begin{equation}
    x_j q_{i|j} [ (k-1)q_{j|i} + 1 + (k-1)q_{j|j} ]
    + x_i q_{j|i} [ (k-1)q_{i|j} + 1 + (k-1)q_{i|i} ]
    + \sum_{m \neq i,j} x_m (k-1)[ q_{i|m}q_{j|i} + q_{j|m}q_{i|j}].
\end{equation}
The $x_m$ term requires a little more care.
Observe that $x_m q_{i|m}q_{j|i} = x_i q_{m|i}q_{j|i} = x_j q_{m|i} q_{i|j}$
and $x_m q_{j|m} q_{i|j} = x_j q_{m|j} q_{i|j}$.
So we have
\begin{equation}
    \begin{split}
        & x_j q_{i|j} [ (k-1)q_{j|i} + 1 + (k-1)q_{j|j} ]
        + x_j q_{i|j}[ (k-1)q_{i|j} + 1 + (k-1)q_{i|i} ]
        + x_j q_{i|j} \sum_{m \neq i,j}  (k-1)[ q_{m|i} + q_{m|j}]
        \\
        = & x_j q_{i|j} [ 2 + (k-1) \overbrace{(q_{j|i} + q_{i|i} + \sum_{m \neq i,j} q_{m|i})}^{\textrm{sums to }1} + (k-1) \underbrace{(q_{j|j} + q_{i|j} + \sum_{m \neq i,j} q_{m|j})}_{\textrm{sums to }1}]
        \\
        = & 2x_j kq_{i|j}.
    \end{split}
    \label{eq:pair_destruction}
\end{equation}
Finally, the possibility of $i = j$ will work exactly the same as in the two-strategy case, except that we'll effectively sum over the possibility of $i$ replacing any $m$ (non-$i$) strategy.
This will require that we add a $\sum_{m \neq i} q_{m|i} = 1 - q_{i|i}$ term (the sum is all that changes from equation \ref{eq:bd_update_ii}), so we'll have another $\delta_{ij}$ to account for that.
Since there are a total of $Nk/2$ pairs, in total we end up with
\begin{equation}
    \begin{split}
        \frac{Nk}{2}\Delta x_{ij} & = x_j \Big[ \delta_{ij} + (k-1)\sum_l \Big( q_{i|l}q_{l|j} \Big) - kq_{i|j} \Big] \nonumber \\
        \therefore \frac{\Delta x_{ij}}{x_j} & = \frac{2}{Nk} \Big[ \delta_{ij} + (k-1)\sum_l \Big( q_{i|l}q_{l|j} \Big) - kq_{i|j} \Big]
        \\
        \therefore \dot{q}_{i|j} = \frac{\dot{x}_{ij}}{x_j} & = \frac{2}{k} \Big[ \delta_{ij} + (k-1)\sum_l \Big( q_{i|l}q_{l|j} \Big) - kq_{i|j} \Big]
    \end{split}
\end{equation}
Observe that nothing changes under different update rules, except that there will be a $k+1$ rather than $k$ term in the ``imitation'' update rule, as an individual can compare itself to itself (there's effectively one extra ``edge'' in the network).

\subsection{Deriving the equilibrium frequencies}

Let's see now if we can use equation \ref{eq:bd_update} to derive the equilibrium frequencies, which \citet{ohtsuki.nowak_2006} claim are given by
\begin{equation}
    q_{i|j} = \frac{(k-2)x_i + \delta_{ij}}{k-1}.
    \label{eq:equilibrium_freqs}
\end{equation}
Observe that there is a $(k-1)$ divisor and a $(k-2)$ prefactor, which probably implies we are going to have to add and subtract something that equals zero in order to get everything to look right.
Observe, further, that this is just the general case of equation 16 from the supplement of \citet{ohtsuki.etal_2006}.
As a sanity check, we could always stick equation \ref{eq:equilibrium_freqs} into equation \ref{eq:bd_update} and verify that it actually equals zero.
Let's do that, first eliminating the $2/k$ prefactor:
\begin{equation}
    \begin{split}
        \frac{k}{2}\dot{q}_{i|j} = 0 & = \delta_{ij} + (k-1)\sum_l \Big( q_{i|l}q_{l|j} \Big) - \frac{(k-2)x_i + \delta_{ij}}{k-1}
        \\
        & = \delta_{ij} + (k-1)\sum_l \Big( \frac{(k-2)x_i + \delta_{il}}{k-1} \frac{(k-2)x_l + \delta_{lj}}{k-1} \Big) - k\frac{(k-2)x_i + \delta_{ij}}{k-1}
        \\
        & = \delta_{ij} + \frac{1}{k-1}\sum_l \Big( [k-2]^2 x_i x_l + [k-2]x_i \delta_{lj} + [k-2]x_l \delta_{il} + \delta_{il} \delta_{lj} \Big) - k\frac{(k-2)x_i + \delta_{ij}}{k-1}
        \\
        & = \delta_{ij} + \frac{1}{k-1} \Big( [k-2]^2 x_i + 2[k-2]x_i + \delta_{ij} \Big) - k\frac{(k-2)x_i + \delta_{ij}}{k-1}
        \\
        & = \frac{(k-1)\delta_{ij} + k(k-2)x_i + \delta_{ij} - k(k-2)x_i - k\delta_{ij}}{k-1}
        \\
        & = 0.
    \end{split}
\end{equation}
We can also argue this verbally (following an explanation by Ohtsuki himself).
Two adjacent individuals $i$ and $j$ on a Bethe lattice share a common ancestor (and hence the same strategy) with probability $r = \delta_{ij}/(k-1)$.
With the remaining probability $1 - r = (k-2)/(k-1)$, a $j$ individual has an (unrelated) neighbor, who is $i$ with a probability given by the global frequency $x_i$.
So we have $q_{i|j}^* = r\delta_{ij} + (1-r)x_i = \frac{(k-2)x_i + \delta_{ij}}{k-1}$, which is just equation \ref{eq:equilibrium_freqs}.

Nonetheless, it should still be possible to derive equation \ref{eq:equilibrium_freqs} directly from equation \ref{eq:bd_update}.
We start with equation \ref{eq:bd_update} and set it equal to zero:
\begin{equation}
    \begin{split}
        \dot{q}_{i|j} = 0 & = \frac{2}{k} \Big[ \delta_{ij} + (k-1)\sum_l \Big( q_{i|l}q_{l|j} \Big) - kq_{i|j} \Big]
        \\
        \therefore 0 & = \delta_{ij} + (k-1)\sum_l \Big( q_{i|l}q_{l|j} \Big) - kq_{i|j}
        \\
        \therefore (k-1)q_{i|j} & = \delta_{ij} + (k-1)\Big( \sum_l q_{i|l}q_{l|j} \Big) - q_{i|j}
        \\
        \therefore q_{i|j} & = \frac{\delta_{ij} + (k-1)\Big( \sum_l q_{i|l}q_{l|j} \Big) - q_{i|j}}{k-1}
    \end{split}
\end{equation}
whereupon we will need to figure out a way to turn $(k-1)(\sum_l q_{i|l}q_{l|j}) - q_{i|j}$ into $(k-2)x_i$.
The only ``hint'' we are given is that $q_{i|j}x_j = q_{j|i}x_i$.
We could expand out $(k-2)x_i = (k-2)\sum_l x_{il} = (k-2)\sum_l q_{i|l} x_l$.

\subsection{The replicator equation with birth-death updating}

We assume now that the conditional frequencies $q_{i|j}$ are in equilibrium and update quickly in response to changes in the global frequencies $x_i$.
In the birth-death updating scheme, an individual is chosen to reproduce with a probability proportional to its fitness, then replaces a random neighbor.

How does $x_i$ change?
It can increase provided that an $i$ individual is chosen to reproduce \emph{and} it replaces a non-$i$ neighbor.
The first event occurs with probability
\begin{equation}
    \Big[ x_i \cdot \overbrace{\Big( \frac{k!}{k_1! k_2! \cdots k_n!} q_{1|i}^{k_1} \cdots q_{n|i}^{k^n} \Big)}^{\textrm{configuration probability}} \cdot \underbrace{W_{(i; k_1, \ldots ,k_n)}}_{\textrm{fitness}} \Big] \Big/ \bar{W},
\end{equation}
in which $W_{(i; k_1, \ldots ,k_n)}$ is the fitness of an $i$ individual with $k_1, \ldots , k_n$ neighbors of each type: $W_{(i; k_1, \ldots ,k_n)} = 1 - w + w \sum_l k_l a_{il}$.
$\bar{W}$ is the mean fitness.
A non-$i$ neighbor is then replaced with probability $1 - k_i/k$.
In order for $x_i$ to decrease, a non-$i$ neighbor must be picked to reproduce, which happens with probability
\begin{equation}
    \Big[ x_j \cdot \overbrace{\Big( \frac{k!}{k_1! k_2! \cdots k_n!} q_{1|j}^{k_1} \cdots q_{n|j}^{k^n} \Big)}^{\textrm{configuration probability}} \cdot \underbrace{W_{(j; k_1, \ldots ,k_n)}}_{\textrm{fitness}} \Big] \Big/ \bar{W},
\end{equation}
and then it must replace an $i$ individual, which happens with probability $k_i/k$.
By subtracting these probabilities, we obtain the expected change in the frequency of $x_i$ in one time step, $\mathbf{E}[\Delta x_i]$.
Dividing by the time step $\Delta t$ yields the time derivative
\begin{equation}
    \begin{split}
        \dot{x}_i = \frac{\mathbf{E}[\Delta x_i]}{\Delta t} & = \sum_{k_1 + \cdots + k_n = k}  \Big[ x_i \cdot \Big( \frac{k!}{k_1! k_2! \cdots k_n!} q_{1|i}^{k_1} \cdots q_{n|i}^{k_n} \Big) \cdot W_{(i; k_1, \ldots ,k_n)} \Big] \cdot \Big(1 - \frac{k_i}{k} \Big)\Big/ \bar{W}
        \\
        - \sum_{j \neq i} & \sum_{k_1 + \cdots + k_n = k} \Big[ x_j \cdot \Big( \frac{k!}{k_1! k_2! \cdots k_n!} q_{1|j}^{k_1} \cdots q_{n|j}^{k_n} \Big) \cdot W_{(j; k_1, \ldots ,k_n)} \Big] \cdot \frac{k_i}{k} \Big/ \bar{W}
        \\
        & \approx w \frac{(k-2)^2}{k-1} \cdot x_i (f_i + g_i - \phi),
    \end{split}
    \label{eq:bd_replicator}
\end{equation}
with
\begin{equation}
    \begin{split}
        f_i & = \sum_j x_j a_{ij},
        \\
        \phi & = \sum_i f_i = \sum_{i,j} x_i x_j a_{ij},
        \\
        g_i & = \sum_j x_j b_{ij},
        \\
        b_{ij} & = \frac{a_{ii} + a_{ij} - a_{ji} - a_{jj}}{k-2}.
    \end{split}
    \label{eq:bd_coeffs}
\end{equation}
This is a mouthful.
Let's try to see how all this works.
The multinomial identities
\begin{equation}
    \begin{split}
        \sum_{k_1 + \cdots + k_n = k}\Big( \frac{k!}{k_1! k_2! \cdots k_n!} q_1^{k_1} \cdots q_n^{k_n} \Big) \cdot k_i &= kq_i,
        \\
        \sum_{k_1 + \cdots + k_n = k}\Big( \frac{k!}{k_1! k_2! \cdots k_n!} q_1^{k_1} \cdots q_n^{k_n} \Big) \cdot k_i k_j &= kq_i\delta_{ij} + k(k-1)q_i q_j
    \end{split}
\end{equation}
will be useful.
($q_i$ behaves like $q_{i|j}$: in particular we have $\sum_l q_l = 1$.)
We will benefit from first figuring out what $\bar{W}$ looks like:
\begin{equation}
    \begin{split}
        \bar{W} &= \sum_m \sum_{k_1 + \cdots + k_n = k} x_m \cdot \Big( \frac{k!}{k_1! k_2! \cdots k_n!} q_{1|m}^{k_1} \cdots q_{n|m}^{k_n} \Big) \cdot W_{(m; k_1, \ldots ,k_n)}
        \\
        &= \sum_m \sum_{k_1 + \cdots + k_n = k} x_m \cdot \Big( \frac{k!}{k_1! k_2! \cdots k_n!} q_{1|m}^{k_1} \cdots q_{n|m}^{k_n} \Big) \cdot \Big( 1 - w + w\sum_l k_l a_{ml} \Big)
        \\
        &= 1 - w + w\sum_m \sum_{k_1 + \cdots + k_n = k} x_m \Big( \frac{k!}{k_1! k_2! \cdots k_n!} q_{1|m}^{k_1} \cdots q_{n|m}^{k_n} \Big) \sum_l k_l a_{ml}
        \\
        &= 1 - w + w\sum_m \sum_{k_1 + \cdots + k_n = k} x_m \sum_l k q_{l|m} a_{ml}
        \\
        &= 1 - w + w\sum_m \sum_{k_1 + \cdots + k_n = k} x_m \sum_l k \frac{(k-2)x_l + \delta_{lm}}{k-1} a_{ml}
        \\
        &= 1 - w + w \Big(\frac{k(k-2)}{k-1} \sum_{m,l} x_m x_l a_{ml} + \frac{k}{k-1} \sum_m x_m a_{mm} \Big)
        \\
        &= 1 - w + w\theta,
    \end{split}
\end{equation}
where
\begin{equation}
    \theta \equiv \frac{k(k-2)}{k-1} \sum_{m,l} x_m x_l a_{ml} + \frac{k}{k-1} \sum_m x_m a_{mm}.
\end{equation}
Thus
\begin{equation}
    \begin{split}
        \frac{W_{(i; k_1, \cdots k_n)}}{\bar{W}} & = \frac{1 - w + w\sum_l k_l a_{il}}{1 - w + w\theta}
        \\
        & \approx 1 + w \Big( \sum_l k_l a_{il} - \theta \Big).
    \end{split}
\end{equation}
The last approximation is just the first term of a Taylor expansion in $w$, given $w \ll 1$: we have
\begin{equation}
    \begin{split}
        f(w) & = \frac{1 - w + wx}{1 - w + wy}
        \\
        & \approx f(0) + w f^\prime(0) + O(w^2)
        \\
        & = 1 + w \frac{(x - 1)(1 - w + wy) - (1 - w + wx)(y - 1)}{(1 - w + wy)^2} + O(w^2)
        \\
        & = 1 + w \frac{x - y}{(1 - w + wy)^2} + O(w^2)
        \\
        & \approx 1 + w(x-y).
    \end{split}
    \label{eq:taylor_trick}
\end{equation}

For the rest of this, I had to cheat a little bit: I asked Ohtsuki, who sent me a writeup that proved to be very useful.
Evidently I am not the first person to be confused by his paper, including several equations that (apparently) took multiple pages of algebra to derive but are, for space reasons, presented without proof.

We can now look term by term at equation \ref{eq:bd_replicator}.
Pulling out the $-k_i / k$ term in the first sum and sticking it inside the second $j \neq i$ sum yields a simpler set of sums:
\begin{equation}
    \begin{split}
        x_i & \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \ldots k_n!} q_{1|i}^{k_1} \ldots q_{n|i}^{k_n} \Big) \cdot \frac{W_{(i; k_1, \ldots ,k_n)}}{\bar{W}}
        \\
        - \sum_j x_j & \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \ldots k_n!} q_{1|j}^{k_1} \ldots q_{n|j}^{k_n} \Big) \cdot \frac{k_i}{k} \frac{W_{(i; k_1, \ldots ,k_n)}}{\bar{W}}.
    \end{split}
    \label{eq:bd_sum}
\end{equation}
The first line of equation \ref{eq:bd_sum} becomes
\begin{equation}
    \begin{split}
        & x_i \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \ldots k_n!} q_{1|i}^{k_1} \ldots q_{n|i}^{k_n} \Big) \cdot \Big[ 1 + w \Big( \sum_l k_l a_{il} - \theta \Big) \Big]
        \\
        = & x_i \Big[ 1 + w \Big( \sum_l kq_{i|l} a_{il} - \theta \Big) \Big]
        \\
        = & x_i \Big[ 1 + w \Big( \sum_l k \frac{(k-2)x_l + \delta_{li}}{k-1} a_{il} - \theta \Big) \Big]
        \\
        = & x_i \Big[ 1 + w \Big( \frac{k(k-2)}{k-1} \sum_l x_l a_{il} + \frac{k}{k-1} a_{ii} - \theta \Big) \Big]
        \\
        = & x_i \Big[ 1 + w \Big( \frac{k}{k-1} a_{ii} + \frac{k(k-2)}{k-1} a_{i \bullet } - \theta \Big) \Big],
    \end{split}
    \label{eq:bd_sum_1}
\end{equation}
in which
\begin{equation}
    \begin{split}
        a_{i \bullet} & = \sum_l x_l a_{il}
        \\
        a_{\bullet i} & = \sum_l x_l a_{li}
        \\
        a_{\bullet} & = \sum_l x_l a_{ll}
        \\
        a_{\bullet \bullet} & = \sum_{l,m} x_l x_m a_{lm}.
    \end{split}
\end{equation}
The second line of equation \ref{eq:bd_sum} becomes
\begin{equation}
    \begin{split}
        & \sum_j x_j \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \ldots k_n!} q_{1|i}^{k_1} \ldots q_{n|i}^{k_n} \Big) \cdot \frac{k_i}{k} \cdot \Big[ 1 + w \Big( \sum_l k_l a_{il} - \theta \Big) \Big]
        \\
        = & \sum_j x_j \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \ldots k_n!} q_{1|i}^{k_1} \ldots q_{n|i}^{k_n} \Big) \cdot \Big[ \frac{k_i}{k} + w \Big( \sum_l \frac{k_i k_l}{k} a_{il} - \frac{k_i}{k}\theta \Big) \Big]
        \\
        = & \sum_j x_j \Big[ \frac{kq_{i|j}}{k} + w \Big( \sum_l \frac{k q_{i|j} \delta_{il} + k(k-1) q_{i|j} q_{l|j}}{k}a_{jl} - \frac{kq_{i|j}}{k}\theta \Big) \Big]
        \\
        = & \sum_j x_j \Big[ q_{i|j} + w \Big( \sum_l \{ q_{i|j} \delta_{il} + (k-1) q_{i|j} q_{l|j} \} a_{jl} - q_{i|j} \theta \Big) \Big]
        \\
        = & \sum_j x_j q_{i|j} \Big[ 1 + w \Big( \sum_l \{ \delta_{il} + (k-1) q_{l|j} \} a_{jl} - \theta \Big) \Big]
        \\
        = & \sum_j x_j \frac{(k-2)x_i + \delta_{ij}}{k-1} \Big[ 1 + w \Big( \sum_l \Big\{ \delta_{il} + (k-1) \frac{(k-2)x_l +\delta_{lj}}{k-1} \Big\} a_{jl} - \theta \Big) \Big]
        \\
        = & \sum_j x_j \frac{(k-2)x_i + \delta_{ij}}{k-1} [ 1 + w(a_{ji} + (k-2) a_{j \bullet} + a_{jj} - \theta )]
        \\
        = & \frac{k-2}{k-1} x_i [ 1 + w(a_{\bullet i} + (k-2) a_{\bullet \bullet} + a_{\bullet} - \theta )]
        + \frac{1}{k-1} x_i [ 1 + w(a_{ii} + (k-2) a_{i \bullet} + a_{ii} - \theta )]
        \\
        = & x_i \Big[ 1 + w \Big( \frac{2}{k-1} a_{ii} + \frac{k-2}{k-1} a_{i \bullet} + \frac{k-2}{k-1} a_{\bullet i} + \frac{(k-1)^2}{k-1} a_{\bullet \bullet} + \frac{k-2}{k-1} a_{\bullet} - \theta \Big) \Big].
    \end{split}
    \label{eq:bd_sum_2}
\end{equation}
Subtracting equation \ref{eq:bd_sum_2} from equation \ref{eq:bd_sum_1} yields
\begin{equation}
    \begin{split}
        & w \frac{(k-2)^2}{k-1} x_i \frac{a_{ii} + (k-1) a_{i \bullet} - a_{\bullet i} - (k-2) a_{\bullet \bullet} - a_{\bullet}}{k-2}
        \\
        = & w \frac{(k-2)^2}{k-1} x_i \Big( a_{i \bullet} + \frac{a_{ii} + a_{i \bullet} - a_{\bullet i} - a_{\bullet}}{k-2} - a_{\bullet \bullet} \Big)
        \\
        = & w \frac{(k-2)^2}{k-1} x_i (f_i + g_i - \phi). \mathrm{~Q.E.D.}
    \end{split}
\end{equation}
Observe that the $\theta$ term canceled out entirely, as did the ``free-floating'' constant terms.
Replicating it line-by-line in the death-birth case might be instructive.

\subsection{The replicator equation with death-birth updating}

In the death-birth case, a random individual is chosen to die, and then its neighbors compete to replace it with a probability proportional to their fitness.
The equilibium conditional frequencies are still given by equation \ref{eq:equilibrium_freqs}.
In order for the frequency of individual $i$ to increase, we require that a non-$i$ (call it $j$) individual be chosen to die and an $i$ individual replace it.
The first event occurs with probability
\begin{equation}
    \sum_{j \neq i} x_j \sum_{k_1 + \cdots + k_n = k}\frac{k!}{k_1! k_2! \cdots k_n!} q_{1|j}^{k_1} \cdots q_{n|j}^{k^n},
\end{equation}
and the second occurs with probability
\begin{equation}
    \frac{k_i W_{i|j}}{\sum_l k_l W_{l|j}},
\end{equation}
where
\begin{equation}
    W_{i|j} = 1 - w + w \cdot \Big( a_{ij} + \sum_l (k-1) q_{l|i} a_{il} \Big)
\end{equation}
is the fitness of an $i$ individual, one of whose neighbors is $j$.
Likewise, the frequency of $i$ decreases if an $i$ individual is chosen to die and a $j$ (non-$i$) individual is chosen to replace it.
The first event occurs with probability
\begin{equation}
    x_i \sum_{k_1 + \cdots + k_n = k}\frac{k!}{k_1! k_2! \cdots k_n!} q_{1|i}^{k_1} \cdots q_{n|i}^{k^n},
\end{equation}
and the second occurs with probability
\begin{equation}
    1 - \frac{k_i W_{i|i}}{\sum_l k_l W_{l|i}}.
\end{equation}
So the overall rate of change of $x_i$ is
\begin{equation}
    \begin{split}
        \dot{x}_i = \sum_{j \neq i} x_j & \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \cdots k_n!} q_{1|j}^{k_1} \cdots q_{n|j}^{k_n} \Big) \cdot \frac{k_i W_{i|j}}{\sum_l k_l W_{l|j}}
        \\
        - x_i & \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \cdots k_n!} q_{1|i}^{k_1} \cdots q_{n|i}^{k_n} \Big) \cdot \Big( 1- \frac{k_i W_{i|i}}{\sum_l k_l W_{l|i}} \Big)
        \\
        & \approx w \frac{(k+1)(k-2)^2}{k (k-1)} \cdot x_i (f_i + g_i - \phi),
    \end{split}
    \label{eq:db_replicator}
\end{equation}
with
\begin{equation}
    \begin{split}
        f_i & = \sum_j x_j a_{ij},
        \\
        \phi & = \sum_i f_i = \sum_{i,j} x_i x_j a_{ij},
        \\
        g_i & = \sum_j x_j b_{ij},
        \\
        b_{ij} & = \frac{(k+1)a_{ii} + a_{ij} - a_{ji} - (k+1)a_{jj}}{(k-1)(k-2)}.
    \end{split}
    \label{eq:db_coeffs}
\end{equation}
Only the $b_{ij}$ differ from equation \ref{eq:bd_coeffs} above, as well as the prefactor.
Let's see if we can work our way back to this.
First, we will probably want to know what $\sum_l k_l W_{l|j}$ looks like:
\begin{equation}
    \begin{split}
        \sum_l k_l W_{l|j} & = \sum_l k_l \Big[ 1 - w + w \cdot \Big( a_{lj} + \sum_m (k-1) q_{m|l} a_{lm} \Big) \Big]
        \\
        & = (1 - w)\sum_l k_l + w \sum_l k_l \Big( a_{lj} + \sum_m (k-1) q_{m|l} a_{lm} \Big)
        \\
        & = (1 - w)k + w \sum_l k_l \Big( a_{lj} + \sum_m (k-1) \frac{(k-2) x_m + \delta_{ml}}{k-1} a_{lm} \Big)
        \\
        & = (1 - w)k + w \sum_l k_l \Big( a_{lj} + \sum_m [(k-2) x_m + \delta_{ml}] a_{lm} \Big)
        \\
        & = (1 - w)k + w \sum_l k_l \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm}  \Big).
    \end{split}
\end{equation}
The ratio $\frac{k_i W_{i|j}}{\sum_l k_l W_{l|j}}$ is
\begin{equation}
    \begin{split}
        \frac{k_i W_{i|j}}{\sum_l k_l W_{l|j}} & = \frac{(1 - w)k_i + w k_i \Big( a_{ij} + \sum_l (k-1) q_{l|i} a_{il} \Big) }{(1 - w)k + w \sum_l k_l \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big) }
        \\
        & = \frac{(1 - w)k_i + w k_i \Big( a_{ij} + a_{ii} + (k-2) \sum_l x_l a_{il} \Big) }{(1 - w)k + w \sum_l k_l \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big) }
        \\
        & \approx \frac{k_i}{k} + w \Big[ k_i \sum_l k_l \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big) - kk_i \Big( a_{ij} + a_{ii} + (k-2) \sum_l x_l a_{il} \Big) \Big] + O(w^2),
        \\
        & = \frac{k_i}{k} + w \Big[ k_i \sum_l k_l \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big) - kk_i \Big( a_{ij} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big) \Big] + O(w^2),
    \end{split}
\end{equation}
rewriting the denominator to avoid using $l$ twice and using a Taylor approximation trick similar to equation \ref{eq:taylor_trick}:
\begin{equation}
    \begin{split}
        f(w) & = \frac{(1 - w)a + wb}{(1 - w)c + wd}
        \\
        & \approx f(0) + w f^\prime(0) + O(w^2)
        \\
        & = \frac{a}{c} + w \frac{ad - bc}{[(1-w)c + wd]^2} + O(w^2)
        \\
        & \approx \frac{a}{c} + w(ad - bc),
    \end{split}
\end{equation}
with $a = k_i$, $b = k_i \Big( a_{ij} + a_{ii} + (k-2) \sum_l x_l a_{il} \Big) $, $c = k$, $d = \sum_l k_l \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{ml} \Big)$, and $w \ll 1$.

We can now try to expand out the terms of equation \ref{eq:db_replicator}.
Note that we can immediately rewrite it as
\begin{equation}
    \begin{split}
        \dot{x}_i = \sum_{j \neq i} x_j & \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \cdots k_n!} q_{1|j}^{k_1} \cdots q_{n|j}^{k_n} \Big) \cdot \frac{k_i W_{i|j}}{\sum_l k_l W_{l|j}}
        \\
        - x_i & \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \cdots k_n!} q_{1|i}^{k_1} \cdots q_{n|i}^{k_n} \Big) \cdot \Big( 1- \frac{k_i W_{i|i}}{\sum_l k_l W_{l|i}} \Big)
        \\
        = \sum_j x_j & \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \cdots k_n!} q_{1|j}^{k_1} \cdots q_{n|j}^{k_n} \Big) \cdot \frac{k_i W_{i|j}}{\sum_l k_l W_{l|j}}
        \\
        - x_i & \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \cdots k_n!} q_{1|i}^{k_1} \cdots q_{n|i}^{k_n} \Big).
    \end{split}
    \label{eq:db_replicator_rearranged}
\end{equation}
We will probably want to reuse the identities
\begin{equation}
    \begin{split}
        a_{i \bullet} & = \sum_l x_l a_{il}
        \\
        a_{\bullet i} & = \sum_l x_l a_{li}
        \\
        a_{\bullet} & = \sum_l x_l a_{ll}
        \\
        a_{\bullet \bullet} & = \sum_{l,m} x_l x_m a_{lm}
    \end{split}
\end{equation}
from above.
The second term quite neatly becomes $-x_i$.
The first term becomes
\begin{equation}
    \begin{split}
        & \sum_j x_j \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \cdots k_n!} q_{1|j}^{k_1} \cdots q_{n|j}^{k_n} \Big) \cdot \frac{k_i W_{i|j}}{\sum_l k_l W_{l|j}}
        \\
        = & \sum_j x_j \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \cdots k_n!} q_{1|j}^{k_1}
        \cdots q_{n|j}^{k_n} \Big)
        \\
        & \cdot \Big\{ \frac{k_i}{k} + w \Big[ k_i \sum_l k_l \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big) - k k_i \Big( a_{ij} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big) \Big] \Big\}
        \\
        = \sum_j x_j q_{i|j} + w & \sum_j x_j \sum_{k_1 + \cdots + k_n = k} \Big( \frac{k!}{k_1! \cdots k_n!} q_{1|j}^{k_1}
        \cdots q_{n|j}^{k_n} \Big)
        \\
        & \cdot \Big[ \sum_l k_i k_l \Big(a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big) - k k_i \Big( a_{ij} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big) \Big]
        \\
        = x_i + w & \sum_j x_j \sum_{k_1 + \cdots + k_n = k} \Big[ \Big( \frac{k!}{k_1! \cdots k_n!} q_{1|j}^{k_1}
        \cdots q_{n|j}^{k_n} \Big) \sum_l k_i k_l \Big(a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big) \Big]
        \\
        - w & \sum_j x_j \sum_{k_1 + \cdots + k_n = k} \Big[ \Big( \frac{k!}{k_1! \cdots k_n!} q_{1|j}^{k_1} \cdots q_{n|j}^{k_n} \Big) k k_i \Big( a_{ij} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big) \Big]
        \\
        = x_i + w & \sum_j x_j \sum_l \Big[ \Big( k q_{i|j} \delta_{il} + k(k-1) q_{i|j}q_{l|j} \Big) \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big) \Big]
        \\
        - w & \sum_j x_j \sum_l k^2 q_{i|j} \Big( a_{lj} + a_{ll}+ (k-2) \sum_m x_m a_{lm} \Big)
        \\
        = x_i + w & \sum_j x_j \Big[ k q_{i|j} \Big( a_{ij} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big) \Big]
        \\
        + w & \sum_j x_j \sum_l \Big[ \Big(k(k-1) q_{i|j}q_{l|j} \Big) \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big) \Big]
        \\
        - w & \sum_j x_j k^2 q_{i|j} \Big( a_{ij} + a_{ii}+ (k-2) \sum_m x_m a_{im} \Big)
        \\
        = x_i + w & \sum_j x_j \Big[ k \frac{(k-2)x_i + \delta_{ij}}{k-1} \Big( a_{ij} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big) \Big]
        \\
        + w & \sum_j x_j \sum_l \Big[ \Big(k(k-1) \frac{(k-2)x_i + \delta_{ij}}{k-1}\frac{(k-2)x_l + \delta_{lj}}{k-1} \Big) \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big) \Big]
        \\
        - w & \sum_j x_j k^2 \frac{(k-2)x_i + \delta_{ij}}{k-1} \Big( a_{ij} + a_{ii}+ (k-2) \sum_m x_m a_{im} \Big)
        \\
        = x_i + w & \sum_j x_j \Big[ k \frac{(k-2)x_i + \delta_{ij}}{k-1} \Big( a_{ij} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big) \Big]
        \\
        + w & \sum_j x_j \sum_l \Big[ \Big(k(k-1) \frac{(k-2)x_i + \delta_{ij}}{k-1}\frac{(k-2)x_l + \delta_{lj}}{k-1} \Big) \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big) \Big]
        \\
        - w & \sum_j x_j k^2 \frac{(k-2)x_i + \delta_{ij}}{k-1} \Big( a_{ij} + a_{ii}+ (k-2) \sum_m x_m a_{im} \Big)
        \\
        = x_i + w & \Big[ x_i k \frac{k-2}{k-1} \Big( 2a_{ii} + (k-2) \sum_m x_m a_{im} \Big) \Big] + w \sum_j x_j \Big[ k\frac{k-2}{k-1} x_i \Big( a_{ij} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big) \Big]
        \\
        + w & \sum_j \sum_l x_j \Big[ k(k-1) \frac{(k-2)^2 x_i x_l + (k-2) x_i \delta_{lj} + (k-2) x_l \delta_{ij} + \delta_{ij}\delta_{lj}}{(k-1)^2} \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big) \Big]
        \\
        - w & \sum_j x_j k^2 \frac{(k-2)x_i + \delta_{ij}}{k-1} \Big( a_{ij} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big).
    \end{split}
    \label{eq:db_sum}
\end{equation}
This is going to quickly become unwieldy unless we are careful. Considering only the first line of the final term of equation \ref{eq:db_sum} and dropping the $x_i$, we have:
\begin{equation}
    \begin{split}
        & w \Big[ x_i k \frac{k-2}{k-1} \Big( 2a_{ii} + (k-2) \sum_m x_m a_{im} \Big) \Big] + w \sum_j x_j \Big[ k\frac{k-2}{k-1} x_i \Big( a_{ij} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big) \Big]
        \\
        = & w \frac{k(k-2)}{k-1} x_i \Big[ \Big(2 a_{ii} + (k-2) \sum_m x_m a_{im} \Big) + \sum_j x_j \Big( a_{ij} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big) \Big]
        \\
        = & w \frac{k(k-2)}{k-1} x_i \Big[ \Big(2 a_{ii} + (k-2) a_{i \bullet} \Big) + a_{i \bullet} + \sum_j x_j a_{ii} + (k-2) \sum_j x_j \sum_m x_m a_{im} \Big) \Big]
        \\
        = & w \frac{k(k-2)}{k-1} x_i \Big[ 2 a_{ii} + (k-2) a_{i \bullet} \Big) + a_{i \bullet} + a_{ii} + (k-2) a_{i \bullet} \Big]
        \\
        = & w \frac{k(k-2)}{k-1} x_i \Big( 3 a_{ii} + (2k-3) a_{i \bullet} \Big).
    \end{split}
\end{equation}
The second line becomes
\begin{equation}
    \begin{split}
        & w \sum_j \sum_l x_j \Big[ k(k-1) \frac{(k-2)^2 x_i x_l + (k-2) x_i \delta_{lj} + (k-2) x_l \delta_{ij} + \delta_{ij}\delta_{lj}}{(k-1)^2} \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big) \Big]
        \\
        = & w \frac{k}{k-1} \sum_j \sum_l x_j (k-2)^2 x_i x_l \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big)
        \\
        + & w \frac{k}{k-1} \sum_j \sum_l x_j (k-2) x_i \delta_{lj} \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big)
        \\
        + & w \frac{k}{k-1} \sum_j \sum_l x_j (k-2) x_l \delta_{ij} \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big)
        \\
        + & w \frac{k}{k-1} \sum_j \sum_l x_j \delta_{ij} \delta_{lj} \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big)
        \\
        = & w \frac{k(k-2)^2}{k-1} x_i \sum_j \sum_l x_j x_l \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big)
        \\
        + & w \frac{k(k-2)}{k-1} x_i \sum_j \sum_l x_j \delta_{lj} \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big)
        \\
        + & w \frac{k(k-2)}{k-1} \sum_j \sum_l x_j x_l \delta_{ij} \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big)
        \\
        + & w \frac{k}{k-1} \sum_j \sum_l x_j \delta_{ij} \delta_{lj} \Big( a_{lj} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big)
        \\
        = & w \frac{k(k-2)^2}{k-1} x_i \Big( a_{\bullet \bullet} + a_{\bullet} + (k-2) a_{\bullet \bullet} \Big)
        \\
        + & w \frac{k(k-2)}{k-1} x_i \sum_l x_l \Big( a_{ll} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big)
        \\
        + & w \frac{k(k-2)}{k-1} x_i \sum_l x_l \Big( a_{li} + a_{ll} + (k-2) \sum_m x_m a_{lm} \Big)
        \\
        + & w \frac{k}{k-1} x_i \Big( a_{ii} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big)
        \\
        = & w \frac{k(k-2)^2}{k-1} x_i \Big( (k-1) a_{\bullet \bullet} + a_{\bullet} \Big) + w \frac{k(k-2)}{k-1} x_i \Big( 2a_{\bullet} + (k-2) a_{\bullet \bullet} \Big)
        \\
        + & w \frac{k(k-2)}{k-1} x_i \Big( a_{\bullet i} + a_{\bullet} + (k-2) a_{\bullet \bullet} \Big) + w \frac{k}{k-1} x_i \Big( 2 a_{ii} + (k-2) a_{\bullet \bullet} \Big).
    \end{split}
\end{equation}
Finally, the third term becomes
\begin{equation}
    \begin{split}
        w \frac{k^2}{k-1} & \sum_j x_j \Big( (k-2)x_i + \delta_{ij} \Big) \Big( a_{ij} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big)
        \\
        = w \frac{k^2}{k-1} & \Big[ (k-2) x_i \sum_j x_j \Big( a_{ij} + a_{ii} + (k-2) \sum_m x_m a_{im} \Big) + x_i \Big( a_{ii} + a_{ll} + (k-2) \sum_m x_m a_{im} \Big) \Big]
        \\
        = w \frac{k^2}{k-1} x_i & \Big[ (k-2) a_{i \bullet} + a_{ii} + (k-2) a_{\bullet \bullet} + 2a_{ii} + a_{i \bullet} \Big].
    \end{split}
\end{equation}
Combining all these (recalling that the third term is to be \emph{subtracted}, not added) will, I'm sure, yield the expected behavior
\begin{equation}
    \begin{split}
        \dot{x}_i = & w \frac{(k+1)(k-2)^2}{k(k-1)} \cdot x_i \Big( a_{i \bullet} + \frac{(k+1)a_{ii} + a_{i \bullet} - a_{\bullet i} - (k+1) a_{\bullet}}{(k+1)(k-2)} - a_{\bullet \bullet} \Big)
        \\
        = & w \frac{(k+1)(k-2)^2}{k(k-1)} \cdot x_i (f_i + g_i - \phi),
    \end{split}
\end{equation}
which is just equation \ref{eq:db_replicator}, though I can't be bothered to actually check right now.

Okay, we might as well check now.
Dropping all the $x_i$ and $w$ terms yields
\begin{equation}
    \begin{split}
        \dot{x}_i \sim & \frac{k(k-2)}{k-1} \Big( 3a_{ii} + (2k - 3) a_{i \bullet} \Big) + \frac{k(k-2)^2}{k-1} \Big( (k-1) a_{\bullet \bullet} + a_{\bullet} \Big) + \frac{k(k-2)}{k-1}\Big( 3a_{\bullet} + a_{\bullet i} + 2(k-2)a_{\bullet \bullet} \Big)
        \\
        + & \frac{k}{k-1} \Big( 2a_{ii} + (k-2) a_{\bullet \bullet} \Big) - \frac{k^2}{k-1} \Big( (k-2) a_{i \bullet} + a_{ii} + (k-2) a_{\bullet \bullet} + 2a_{ii} + a_{i \bullet} \Big).
    \end{split}
\end{equation}
The desired prefactor is $(k+1)(k-2)^2/k(k-1)$, so we have
\begin{equation}
    \begin{split}
        \dot{x}_i \sim & \frac{(k+1)(k-2)^2}{k(k-1)} \Big( \frac{(k+1)(k-2)}{k} 3a_{ii} + \frac{k}{(k+1)(k-2)} (2k-3) a_{i \bullet} \Big) \ldots
    \end{split}
\end{equation}
%
% As before, we'll start with the two strategy case.
% Then equation \ref{eq:bd_replicator} simplifies to
% \begin{equation}
%     \begin{split}
%         \dot{x}_1 = \frac{\mathbf{E}[\Delta x_1]}{\Delta t} & = \sum_{k_1 + k_2 = k}  \Big[ x_1 \cdot \Big( \frac{k!}{k_1!k_2!} q_{1|1}^{k_1} q_{2|1}^{k_2} \Big) \cdot W_{(1; k_1, k_2)} \Big] \cdot \Big(1 - \frac{k_1}{k} \Big)\Big/ \bar{W}
%         \\
%         & - \sum_{k_1 + k_2 = k} \Big[ x_2 \cdot \Big( \frac{k!}{k_1! k_2!} q_{1|2}^{k_1} q_{2|2}^{k_2} \Big) \cdot W_{(2; k_1, k_2)} \Big] \cdot \frac{k_1}{k} \Big/ \bar{W},
%     \end{split}
%     \label{eq:bd_replicator_ij}
% \end{equation}
% with
% \begin{equation}
%     \begin{split}
%         W_{(1; k_1, k_2)} & = 1 - w + w(k_1 a_{11} + k_2 a_{12}),
%         \\
%         W_{(2; k_1, k_2)} & = 1 - w + w(k_1 a_{21} + k_2 a_{22}),
%         \\
%         q_{1|1} & = \frac{(k-2)x_1 + 1}{k-1},
%         \\
%         q_{1|2} & = \frac{(k-2)x_1}{k-1},
%         \\
%         q_{2|1} & = \frac{(k-2)x_2}{k-1},
%         \\
%         q_{2|2} & = \frac{(k-2)x_2 + 1}{k-1}.
%     \end{split}
% \end{equation}
% We might as well see if we can combine all of equation \ref{eq:bd_replicator_ij} into one sum.
% %\begin{equation}
% \begin{alignat*}{2}
%     \dot{x}_1 = & \frac{1}{\bar{W}} \sum_{k_1 + k_2 = k} & \Bigg\{ & \Big[ x_1 \cdot \Big( \frac{k!}{k_1!k_2!} q_{1|1}^{k_1} q_{2|1}^{k_2} \Big) \cdot W_{(1; k_1, k_2)} \Big] \cdot \Big(1 - \frac{k_1}{k} \Big)
%     \\
%     && - &\Big[ x_2 \cdot \Big( \frac{k!}{k_1! k_2!} q_{1|2}^{k_1} q_{2|2}^{k_2} \Big) \cdot W_{(2; k_1, k_2)} \Big] \cdot \frac{k_1}{k} \Bigg\}
%     \\
%     = & \frac{1}{\bar{W}(k-1)^k} \sum_{k_1 + k_2 = k} & \Bigg\{ & \Big[ x_1 \cdot \Big( \frac{k!}{k_1!k_2!} [(k-2)x_1+1]^{k_1}[(k-2)x_1]^{k_2} \Big) \cdot (1 - w + w[k_1 a_{11} + k_2 a_{12}]) \Big] \cdot \Big(1 - \frac{k_1}{k} \Big)
%     \\
%     && - & \Big[ x_2 \cdot \Big( \frac{k!}{k_1! k_2!} [(k-2)x_2]^{k_1}[(k-2)x_2 + 1]^{k_2} \Big) \cdot (1 - w + w[k_1 a_{21} + k_2 a_{22}]) \Big] \cdot \frac{k_1}{k} \Bigg\}
%     \\
%     = & \frac{1}{\bar{W}(k-1)^k} \sum_{k_1 + k_2 = k} & \Bigg\{ & \Big[ x_1 \cdot \Big( \frac{k!}{k_1!k_2!} [(k-2)x_1+1]^{k_1}[(k-2)x_1]^{k_2} \Big) \cdot (1 - w + w[k_1 a_{11} + k_2 a_{12}]) \Big]
%     \\
%     && - & \Big[ x_1 \cdot \Big( \frac{k!}{k_1!k_2!} [(k-2)x_1+1]^{k_1}[(k-2)x_1]^{k_2} \Big) \cdot (1 - w + w[k_1 a_{11} + k_2 a_{12}]) \Big] \cdot \frac{k_1}{k}
%     \\
%     && - & \Big[ x_2 \cdot \Big( \frac{k!}{k_1! k_2!} [(k-2)x_2]^{k_1}[(k-2)x_2 + 1]^{k_2} \Big) \cdot (1 - w + w[k_1 a_{21} + k_2 a_{22}]) \Big] \cdot \frac{k_1}{k} \Bigg\}
%     \label{eq:bd_replicator_12}
% \end{alignat*}
%\end{equation}

\bibliographystyle{plainnat}
\bibliography{notes_bib}
\end{document}
